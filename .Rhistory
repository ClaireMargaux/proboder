# Compute the different scores
score_SPE <- squared_prediction_error(U_plot, df_beta)
score_NLPD <- negative_log_predictive_density(U_plot, df_beta)
score_CRPS <- continuous_ranked_probability_score(U_plot, df_beta)
# Store results (during grid search)
results_grid_search[i, ] <- c(param_i$l, param_i$noise_wiener_X, param_i$noise_wiener_U, param_i$beta0prime, score_SPE, score_NLPD, score_CRPS)
# Update progress bar
pb$tick()
} # End of grid search
# Extract the parameter values
params <- results_grid_search[, 1:num_params]
# Extract the score vectors
SPE_scores <- results_grid_search[, (num_params + 1):(num_params + num_predictions)]
NLPD_scores <- results_grid_search[, (num_params + num_predictions + 1):(num_params + 2 * num_predictions)]
CRPS_scores <- results_grid_search[, (num_params + 2 * num_predictions + 1):(num_params + 3 * num_predictions)]
# Calculate the mean and median for each score
mean_SPE <- rowMeans(SPE_scores)
median_SPE <- apply(SPE_scores, 1, median)
mean_NLPD <- rowMeans(NLPD_scores)
median_NLPD <- apply(NLPD_scores, 1, median)
mean_CRPS <- rowMeans(CRPS_scores)
median_CRPS <- apply(CRPS_scores, 1, median)
# Combine into a new data frame
results_summary <- data.frame(
l = params[, 1],
noise_wiener_X = params[, 2],
noise_wiener_U = params[, 3],
beta0prime = params[, 4],
mean_SPE = mean_SPE,
median_SPE = median_SPE,
mean_NLPD = mean_NLPD,
median_NLPD = median_NLPD,
mean_CRPS = mean_CRPS,
median_CRPS = median_CRPS
)
# Total computation time
toc()
return(results_summary)
}
results_summary <-
run_random_search(
model = model, obs_to_use = obs_to_use, df_beta = df_beta,
noise = noise, lambda = lambda, gamma = gamma, eta = eta, pop = pop,
beta0 = beta0, jit = TRUE,
seed = seed, num_param_sets = num_param_sets,
seq_l = seq_l,
seq_wiener_X = seq_wiener_X,
seq_wiener_U = seq_wiener_U,
seq_beta0prime = seq_beta0prime
)
# Run inference
inference_results_i <- inference(model = model,
grids = grids,
obs = obs_to_use,
jit = jit,
initial_params = initial_params)
# Process data for scoring and visualization
print(inference_results_i)
processed_data <- process_data(inference_results_i, grids)
processed_data <- process_data(inference_results_i, grids)
View(inference)
View(jacobian_f)
View(process_data)
#' Process Data
#'
#' This function processes the inferred results for visualization.
#'
#' @param inference_results List of the inferred results.
#' @param grids List of time grids used for inference.
#' @return A list containing processed data for visualization.
#' @export
process_data <- function(inference_results, grids) {
# Get results and time grid
X_values <- inference_results$X_values
U_values <- inference_results$U_values
P_X_values <- inference_results$P_X_values
P_U_values <- inference_results$P_U_values
time_grid <- grids$time_grid
# Process U_values for visualization
U_value <- U_values[1, ]
U_scaled <- sigmoid(U_value)
# Process P_U_values for visualization
P_U <- P_U_values[1, 1, ]
P_U_scaled <- sigmoid(P_U)
# Process X_values for visualization
X <- t(X_values[1:5, ])
X_val <- data.frame(time = time_grid, X=X, row.names = NULL)
colnames(X_val) <- c("time_grid","S","E","I","R","D")
# Process P_X_values for visualization
P_X <- P_X_values[1:5,1:5,]
# Generate values for error area
calculate_y_bounds <- function(X, P_X, sigma) {
# Replace negative values in P_X with 0
P_X[P_X < 0] <- 0
# Calculate ymin and ymax
ymin <- mapply(function(mu, sigma) (qnorm(0.025, mean = mu, sd = sqrt(sigma))), X, P_X)
ymax <- mapply(function(mu, sigma) (qnorm(0.975, mean = mu, sd = sqrt(sigma))), X, P_X)
return(list(ymin = ymin, ymax = ymax))
}
ymin_S <- calculate_y_bounds(X_val$S,P_X[1,1,])[[1]]
ymax_S <- calculate_y_bounds(X_val$S,P_X[1,1,])[[2]]
ymin_E <- calculate_y_bounds(X_val$E,P_X[2,2,])[[1]]
ymax_E <- calculate_y_bounds(X_val$E,P_X[2,2,])[[2]]
ymin_I <- calculate_y_bounds(X_val$I,P_X[3,3,])[[1]]
ymax_I <- calculate_y_bounds(X_val$I,P_X[3,3,])[[2]]
ymin_R <- calculate_y_bounds(X_val$R,P_X[4,4,])[[1]]
ymax_R <- calculate_y_bounds(X_val$R,P_X[4,4,])[[2]]
ymin_D <- calculate_y_bounds(X_val$D,P_X[5,5,])[[1]]
ymax_D <- calculate_y_bounds(X_val$D,P_X[5,5,])[[2]]
minmax <- data.frame(
ymin_S = ymin_S,
ymax_S = ymax_S,
ymin_E = ymin_E,
ymax_E = ymax_E,
ymin_I = ymin_I,
ymax_I = ymax_I,
ymin_R = ymin_R,
ymax_R = ymax_R,
ymin_D = ymin_D,
ymax_D = ymax_D
)
ymin_U <- mapply(function(mu, sigma) sigmoid(qnorm(0.025, mean = mu, sd = sqrt(sigma))), U_value, P_U)
ymax_U <- mapply(function(mu, sigma) sigmoid(qnorm(0.975, mean = mu, sd = sqrt(sigma))), U_value, P_U)
# Create a data frame for the plot of the contact rate
U_plot <- data.frame(t = time_grid, U_scaled = U_scaled, ymin = ymin_U, ymax = ymax_U, P_U_scaled = P_U_scaled, row.names = NULL)
# Create data frame for the plot of the compartments
X_plot <- data.frame(X_val = X_val, minmax = minmax, row.names = NULL)
colnames(X_plot) <- c('t','S','E','I','R','D',
'minS','maxS',
'minE','maxE',
'minI','maxI',
'minR','maxR',
'minD','maxD')
# Return processed data
return(list(U_plot = U_plot, X_plot = X_plot))
}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=6)
# Set source directory
source_directory <- '~/Documents/GitHub/proboder/Functions/'
# Source necessary functions
source(file.path(source_directory, 'LSODA.R'))
source(file.path(source_directory, 'initialization.R'))
source(file.path(source_directory, 'inference.R'))
source(file.path(source_directory, 'processing_saving_loading.R'))
source(file.path(source_directory, 'scoring.R'))
source(file.path(source_directory, 'plotting.R'))
source(file.path(source_directory, 'random_search.R'))
# Load or install necessary packages
load_required_packages()
# --------------
# Initialization
# --------------
model <- 'SEIR' # model to be simulated ('SEIRD' or 'SEIR' available)
steps <- 1 # steps for time grid
max_time <- 30 # max time for time grid
lambda <- 1/(2.6) # latency rate, ref value 1/(2.6)
gamma <- 1/(2.6) # recovery rate, ref value 1/(2.6)
if (model == 'SEIRD') {
eta <- 0.024*(1/15) # fatality rate, ref value 0.024*(1/15)
} else if (model == 'SEIR') {
eta <- 0
}
beta <- function(t){2*cos(t/30)-1} # funtion for contact rate
# Starting compartment counts
if (model == 'SEIRD') {
xstart <- c(S = 499980, E = 5, I = 5, R = 5, D = 0)
} else if (model == 'SEIR') {
xstart <- c(S = 499985, E = 5, I = 5, R = 5)
}
pop <- sum(xstart)
noise <- 10 # noise to add on the data, set > 0 to avoid numerical instabilities
seed <- 5 # any integer, for reproducibility (set NA for no seed)
# ------------------------
# Simulation of a data set
# ------------------------
# Data simulation
sim <- simulate_data_LSODA(
model = model,
noise = noise,
seed = seed,
steps = steps,
max_time = max_time,
lambda = lambda, gamma = gamma, eta = eta,
pop = pop,
beta = beta,
xstart = xstart)
# Get simulated data sets
obs <- sim$obs
df_beta <- sim$df_beta
if (noise > 0) {
obs_with_noise <- sim$obs_with_noise
}
# Visualization
plots <- plotting_simulated_data_lsoda(
model = model,
sim = sim,
latency_rate = lambda,
recovery_rate = gamma,
fatality_rate = eta,
log = TRUE)
(simulated_compartments <- plots$simulated_compartments)
(simulated_beta <- plots$simulated_beta)
l <- 9 # lengthscale
noise_wiener_X <- 50 # noise of the Wiener process associated with X
noise_wiener_U <- 0.01 # noise of the Wiener process associated with U
beta0 = df_beta$beta[1] # starting contact rate
beta0prime <- 0 # starting 1st derivative of contact rate
obs_to_use <- obs_with_noise # choose if obs or obs_with_noise
# Initialize
initial_params <-
initialization(model = model, obs = obs_to_use,
beta0 = beta0, beta0prime = beta0prime,
lambda = lambda, gamma = gamma, eta = eta,
l = l, scale = 1, noise_obs = noise,
noise_X = sqrt(noise), noise_U = 0.01,
noise_wiener_X = noise_wiener_X,
noise_wiener_U = noise_wiener_U,
pop = pop)
# Generate time grids for inference
grids <- generate_grid(obs, num_points_between = 0)
# Run inference
inference_results <- inference(model = model,
grids = grids,
obs = obs,
jit = TRUE,
initial_params = initial_params)
# Process data for scoring and visualization
processed_data <- process_data(inference_results,grids)
U_plot <- processed_data$U_plot
X_plot <- processed_data$X_plot
# Compute and display scores
(styled_table <-
compute_scores_and_table(U_plot = U_plot, df_beta = df_beta))
# Plot compartment counts inferred from simulated data
(compartments <-
plot_compartments(model = model,
obs = obs,
obs_with_noise = obs_with_noise, # if available, otherwise set NULL
X_plot = X_plot))
# Plot compartment counts separately
plots_sep <-
plot_compartments_separately(
model = model,
obs = obs,
obs_with_noise = NULL, # if available, otherwise set NULL
X_plot = X_plot)
num_plots <- length(plots_sep)
invisible(grid_plots_sep <- grid.arrange(
grobs = plots_sep,
ncol = 2,
nrow = ceiling(num_plots/2)))
# Plot contact rate with 95% confidence interval
(contact_rate_with_CI <- plot_contact_rate_with_CI(U_plot = U_plot,
df_beta = df_beta, # if available, otherwise set NULL
latency_rate = lambda,
recovery_rate = gamma,
fatality_rate = eta,
lengthscale = l))
# If wished, a random search on a set of parameters can be performed
if (TRUE) { # set TRUE to perform random search
seed <- 5
num_param_sets <- 10 # Number of parameter sets to sample.
seq_l <- seq(6, 9, by = 0.05)
seq_wiener_X <- seq(20, 1000, by = 25)
seq_wiener_U <- seq(0.005, 1, by = 0.005)
seq_beta0prime <- c(-0.5, 0, 0.5)
results_summary <-
run_random_search(
model = model, obs_to_use = obs_to_use, df_beta = df_beta,
noise = noise, lambda = lambda, gamma = gamma, eta = eta, pop = pop,
beta0 = beta0, jit = TRUE,
seed = seed, num_param_sets = num_param_sets,
seq_l = seq_l,
seq_wiener_X = seq_wiener_X,
seq_wiener_U = seq_wiener_U,
seq_beta0prime = seq_beta0prime
)
plots_scores <- plot_scores(results_summary)
all_plots <- c(plots_scores$mean_plots, plots_scores$median_plots)
invisible(grid_of_plots <- grid.arrange(
grobs = all_plots,
ncol = 2,
nrow = 4))
(best_params <- find_best_hyperparameters(results_summary))
}
# If wished, a random search on a set of parameters can be performed
if (TRUE) { # set TRUE to perform random search
seed <- 5
num_param_sets <- 1000 # Number of parameter sets to sample.
seq_l <- seq(6, 9, by = 0.05)
seq_wiener_X <- seq(20, 1000, by = 25)
seq_wiener_U <- seq(0.005, 1, by = 0.005)
seq_beta0prime <- c(-0.5, 0, 0.5)
results_summary <-
run_random_search(
model = model, obs_to_use = obs_to_use, df_beta = df_beta,
noise = noise, lambda = lambda, gamma = gamma, eta = eta, pop = pop,
beta0 = beta0, jit = TRUE,
seed = seed, num_param_sets = num_param_sets,
seq_l = seq_l,
seq_wiener_X = seq_wiener_X,
seq_wiener_U = seq_wiener_U,
seq_beta0prime = seq_beta0prime
)
plots_scores <- plot_scores(results_summary)
all_plots <- c(plots_scores$mean_plots, plots_scores$median_plots)
invisible(grid_of_plots <- grid.arrange(
grobs = all_plots,
ncol = 2,
nrow = 4))
(best_params <- find_best_hyperparameters(results_summary))
}
# Set source directory
source_directory <- '~/Documents/GitHub/proboder/Functions/'
# Source necessary functions
source(file.path(source_directory, 'LSODA.R'))
source(file.path(source_directory, 'initialization.R'))
source(file.path(source_directory, 'inference.R'))
source(file.path(source_directory, 'processing_saving_loading.R'))
source(file.path(source_directory, 'scoring.R'))
source(file.path(source_directory, 'plotting.R'))
source(file.path(source_directory, 'random_search.R'))
# Load or install necessary packages
load_required_packages()
# If wished, a random search on a set of parameters can be performed
if (TRUE) { # set TRUE to perform random search
seed <- 5
num_param_sets <- 1000 # Number of parameter sets to sample.
seq_l <- seq(6, 9, by = 0.05)
seq_wiener_X <- seq(20, 1000, by = 25)
seq_wiener_U <- seq(0.005, 1, by = 0.005)
seq_beta0prime <- c(-0.5, 0, 0.5)
results_summary <-
run_random_search(
model = model, obs_to_use = obs_to_use, df_beta = df_beta,
noise = noise, lambda = lambda, gamma = gamma, eta = eta, pop = pop,
beta0 = beta0, jit = TRUE,
seed = seed, num_param_sets = num_param_sets,
seq_l = seq_l,
seq_wiener_X = seq_wiener_X,
seq_wiener_U = seq_wiener_U,
seq_beta0prime = seq_beta0prime
)
plots_scores <- plot_scores(results_summary)
all_plots <- c(plots_scores$mean_plots, plots_scores$median_plots)
invisible(grid_of_plots <- grid.arrange(
grobs = all_plots,
ncol = 2,
nrow = 4))
(best_params <- find_best_hyperparameters(results_summary))
}
l <- 9 # lengthscale
noise_wiener_X <- 50 # noise of the Wiener process associated with X
noise_wiener_U <- 0.01 # noise of the Wiener process associated with U
beta0 = df_beta$beta[1] # starting contact rate
beta0prime <- 0 # starting 1st derivative of contact rate
obs_to_use <- obs_with_noise # choose if obs or obs_with_noise
# Initialize
initial_params <-
initialization(model = model, obs = obs_to_use,
beta0 = beta0, beta0prime = beta0prime,
lambda = lambda, gamma = gamma, eta = eta,
l = l, scale = 1, noise_obs = noise,
noise_X = sqrt(noise), noise_U = 0.01,
noise_wiener_X = noise_wiener_X,
noise_wiener_U = noise_wiener_U,
pop = pop)
# Generate time grids for inference
grids <- generate_grid(obs, num_points_between = 0)
# Run inference
inference_results <- inference(model = model,
grids = grids,
obs = obs,
jit = TRUE,
initial_params = initial_params)
# Process data for scoring and visualization
processed_data <- process_data(inference_results,grids)
U_plot <- processed_data$U_plot
X_plot <- processed_data$X_plot
# Compute and display scores
(styled_table <-
compute_scores_and_table(U_plot = U_plot, df_beta = df_beta))
# Plot compartment counts inferred from simulated data
(compartments <-
plot_compartments(model = model,
obs = obs,
obs_with_noise = obs_with_noise, # if available, otherwise set NULL
X_plot = X_plot))
# Plot compartment counts separately
plots_sep <-
plot_compartments_separately(
model = model,
obs = obs,
obs_with_noise = NULL, # if available, otherwise set NULL
X_plot = X_plot)
num_plots <- length(plots_sep)
invisible(grid_plots_sep <- grid.arrange(
grobs = plots_sep,
ncol = 2,
nrow = ceiling(num_plots/2)))
# Plot contact rate with 95% confidence interval
(contact_rate_with_CI <- plot_contact_rate_with_CI(U_plot = U_plot,
df_beta = df_beta, # if available, otherwise set NULL
latency_rate = lambda,
recovery_rate = gamma,
fatality_rate = eta,
lengthscale = l))
best_params
# If wished, the processed data can be stored
directory_save <- '~/Documents/GitHub/proboder/Results/'
if (TRUE) { # Set TRUE to save
save_processed_data(inference_results = inference_results,
processed_data = processed_data,
directory = directory_save,
simulated_compartments = simulated_compartments,
simulated_beta = simulated_beta,
compartments = compartments,
contact_rate_with_CI = contact_rate_with_CI,
grid_plots_sep = grid_plots_sep,
styled_table = styled_table)
}
# To store the table as a png file, use the manual export option from the viewer.
# Save in size 400x200.
best_params
l <- 6.15 # lengthscale
noise_wiener_X <- 20 # noise of the Wiener process associated with X
noise_wiener_U <- 0.055 # noise of the Wiener process associated with U
beta0 = df_beta$beta[1] # starting contact rate
beta0prime <- 0 # starting 1st derivative of contact rate
obs_to_use <- obs_with_noise # choose if obs or obs_with_noise
# Initialize
initial_params <-
initialization(model = model, obs = obs_to_use,
beta0 = beta0, beta0prime = beta0prime,
lambda = lambda, gamma = gamma, eta = eta,
l = l, scale = 1, noise_obs = noise,
noise_X = sqrt(noise), noise_U = 0.01,
noise_wiener_X = noise_wiener_X,
noise_wiener_U = noise_wiener_U,
pop = pop)
# Generate time grids for inference
grids <- generate_grid(obs, num_points_between = 0)
# Run inference
inference_results <- inference(model = model,
grids = grids,
obs = obs,
jit = TRUE,
initial_params = initial_params)
# Process data for scoring and visualization
processed_data <- process_data(inference_results,grids)
U_plot <- processed_data$U_plot
X_plot <- processed_data$X_plot
# Compute and display scores
(styled_table <-
compute_scores_and_table(U_plot = U_plot, df_beta = df_beta))
# Plot compartment counts inferred from simulated data
(compartments <-
plot_compartments(model = model,
obs = obs,
obs_with_noise = obs_with_noise, # if available, otherwise set NULL
X_plot = X_plot))
# Plot compartment counts separately
plots_sep <- plot_compartments_separately(model = model,
obs = obs,
obs_with_noise = NULL, # if available, otherwise set NULL
X_plot = X_plot)
num_plots <- length(plots_sep)
invisible(grid_plots_sep <- grid.arrange(
grobs = plots_sep,
ncol = 2,
nrow = ceiling(num_plots/2)))
# Plot contact rate with 95% confidence interval
(contact_rate_with_CI <- plot_contact_rate_with_CI(U_plot = U_plot,
df_beta = df_beta, # if available, otherwise set NULL
latency_rate = lambda,
recovery_rate = gamma,
fatality_rate = eta,
lengthscale = l))
# If wished, the processed data can be stored
directory_save <- '~/Documents/GitHub/proboder/Results/'
if (TRUE) { # Set TRUE to save
save_processed_data(inference_results = inference_results,
processed_data = processed_data,
directory = directory_save,
simulated_compartments = simulated_compartments,
simulated_beta = simulated_beta,
compartments = compartments,
contact_rate_with_CI = contact_rate_with_CI,
grid_plots_sep = grid_plots_sep,
styled_table = styled_table)
}
# To store the table as a png file, use the manual export option from the viewer.
# Save in size 400x200.
# If desired, save the plot using ggsave
ggsave(filename = file_path('~/Documents/GitHub/proboder/Results/2024-07-05_16-30-05-sim-A/','grid_search.png'),
plot = all_plots,
bg = "white",
width = 8,
height = 6)
# If desired, save the plot using ggsave
output_dir <- '~/Documents/GitHub/proboder/Results/2024-07-05_16-30-05-sim-A/'
ggsave(filename = file.path(output_dir, 'grid_search.png'),
plot = all_plots,
bg = "white",
width = 8,
height = 6)
ggsave(filename = file.path(output_dir, 'grid_search.png'),
plot = grid_of_plots,
bg = "white",
width = 8,
height = 6)
best_params
